{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import json\n",
    "#import pydevd\n",
    "#pydevd.settrace('localhost', port=49309, stdoutToServer=True, stderrToServer=True)\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, defaultdict\n",
    "from six.moves import xrange\n",
    "import pynauty\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pynauty as nauty\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "import os\n",
    "\n",
    "import Dataset, utils\n",
    "\n",
    "# ### Load data\n",
    "'''\n",
    "#Node id is made to start from 0 due to nauty package requirement, even if it starts from 1 in the original\n",
    "#Graph id is starting from 1\n",
    "'''\n",
    "\n",
    "mutag = Dataset.Dropbox('MUTAG')\n",
    "df_edge_label = mutag.get_edge_label()\n",
    "df_graph_ind = mutag.get_graph_ind()\n",
    "df_adj = mutag.get_adj()\n",
    "df_node_label = mutag.get_node_label()\n",
    "df_node_label = pd.concat([df_node_label, df_graph_ind.graph_ind], axis=1)\n",
    "del df_graph_ind\n",
    "\n",
    "\n",
    "\n",
    "# ### Functions\n",
    "\n",
    "\n",
    "def get_subset_adj(df_adj, df_node_label,graph_label_num):\n",
    "    df_glabel = df_node_label[df_node_label.graph_ind == graph_label_num ]\n",
    "    index_of_glabel = (df_adj['to'].isin(df_glabel.node) & df_adj['from'].isin(df_glabel.node))\n",
    "    return df_adj[index_of_glabel]\n",
    "\n",
    "def get_smallest_node_id_from_adj(df_adj):\n",
    "    return min(df_adj['to'].min(), df_adj['from'].min())\n",
    "\n",
    "\n",
    "def create_adj_dict_by_graphId(df_adj, df_node_label):\n",
    "    '''\n",
    "    input: df_node_label\n",
    "    return: {1: {0:[0,2,5]}} = {graphId: {nodeId:[node,node,node]}}\n",
    "    '''\n",
    "    adj_dict_by_graphId ={}\n",
    "    unique_graph_labels = df_node_label.graph_ind.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = get_subset_adj(df_adj, df_node_label, graph_label_num=l)\n",
    "        smallest_node_id = get_smallest_node_id_from_adj(df_subset_adj)\n",
    "        df_subset_adj -= smallest_node_id\n",
    "        adj_dict_by_graphId[l] = df_subset_adj\n",
    "    return adj_dict_by_graphId\n",
    "\n",
    "\n",
    "def canonical_labeling(adj_dict_by_graphId, df_node_label, df_adj):\n",
    "    all_canonical_labels =[]\n",
    "    unique_graph_labels = df_node_label.graph_ind.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = adj_dict_by_graphId[l]\n",
    "        df_subset_nodes = df_node_label[df_node_label.graph_ind==l]\n",
    "        temp_graph_dict = utils.dfadj_to_dict(df_subset_adj)\n",
    "        nauty_graph = nauty.Graph(len(temp_graph_dict), adjacency_dict=temp_graph_dict)\n",
    "        canonical_labeling = nauty.canonical_labeling(nauty_graph)\n",
    "        canonical_labeling = [df_subset_nodes.label.values[i] for i in canonical_labeling] ###\n",
    "        all_canonical_labels += canonical_labeling\n",
    "    return all_canonical_labels\n",
    "\n",
    "\n",
    "def create_adj_coomatrix_by_graphId(adj_dict_by_graphId, df_node_label):\n",
    "    \"\"\"\n",
    "    return: a coomatrix per graphId\n",
    "    \"\"\"\n",
    "\n",
    "    adj_coomatrix_by_graphId ={}\n",
    "    unique_graph_labels = df_node_label.graph_ind.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = adj_dict_by_graphId[l]\n",
    "        df_subset_node_label = df_node_label[df_node_label.graph_ind == l]\n",
    "        adjacency = coo_matrix(( np.ones(len(df_subset_adj)),\n",
    "                                (df_subset_adj.iloc[:,0].values, df_subset_adj.iloc[:,1].values) ),\n",
    "                                 shape=(len(df_subset_node_label), len(df_subset_node_label))\n",
    "                              )\n",
    "        adj_coomatrix_by_graphId[l]=adjacency\n",
    "    return adj_coomatrix_by_graphId\n",
    "\n",
    "def make_neighbor(adj_coomatrix_by_graphId, df_node_label, WIDTH_W, RECEPTIVE_FIELD_SIZE_K):\n",
    "\n",
    "    \"\"\"\n",
    "    return: a dictionary with the shape of {graphId:[matrix: node x neighbor]}\n",
    "    The size of 2D matrix is (Node number) x (RECEPTIVE_FIELD_SIZE_K).\n",
    "    \"\"\"\n",
    "    neighborhoods_dict=dict()\n",
    "    unique_graph_labels = df_node_label.graph_ind.unique()\n",
    "    for l_ind, l in enumerate(unique_graph_labels):\n",
    "        adjacency = adj_coomatrix_by_graphId[l]\n",
    "        graph = nx.from_numpy_matrix(adjacency.todense())\n",
    "\n",
    "        # Create the neighbors with -1 for neighbor assemble.\n",
    "        #After this, if the RECEPTIVE_FIELD_SIZE_K exceeds the number of WIDTH_W, then fill them with -1\n",
    "        neighborhoods = np.zeros((WIDTH_W, RECEPTIVE_FIELD_SIZE_K), dtype=np.int32)\n",
    "        neighborhoods.fill(-1)\n",
    "\n",
    "        df_sequence = df_node_label[df_node_label.graph_ind == l]\n",
    "        df_sequence = df_sequence.sort_values(by='cano_label')\n",
    "        smallest_node_id = df_sequence.node.min()\n",
    "\n",
    "        # CUT GRAPH BY THRESHOLD of cano_label ''' Top width w elements of V according to labeling  '''\n",
    "        df_sequence = df_sequence.iloc[:WIDTH_W,:]\n",
    "        df_sequence['node'] = df_sequence.node.values  - smallest_node_id\n",
    "\n",
    "        for i, node in enumerate(df_sequence.node):\n",
    "            #shortest = nx.single_source_dijkstra_path_length(graph, node).items()\n",
    "            df_shortest = pd.DataFrame.from_dict(nx.single_source_dijkstra_path_length(graph, node),\n",
    "                                                 orient='index') #\n",
    "            df_shortest.columns =['distance'] #\n",
    "            df_shortest['node'] = df_shortest.index.values #\n",
    "            df_shortest = pd.merge(df_node_label, df_shortest, on='node', how='right') #\n",
    "\n",
    "            # Sort by distance and then by cano_label\n",
    "            df_shortest = df_shortest.sort_values(by=['distance','cano_label']) #\n",
    "            df_shortest = df_shortest.iloc[:RECEPTIVE_FIELD_SIZE_K,:] #\n",
    "            #shortest = sorted(shortest, key=lambda v: v[1])\n",
    "            #shortest = shortest[:RECEPTIVE_FIELD_SIZE_K]\n",
    "            for j in range(0, min(RECEPTIVE_FIELD_SIZE_K, len(df_shortest))):\n",
    "                #neighborhoods[i][j] = shortest[j][0]\n",
    "                neighborhoods[i][j] = df_shortest['node'].values[j] + smallest_node_id\n",
    "\n",
    "        #neighborhoods_dict[l]= neighborhoods.copy()\n",
    "        if l_ind ==0: neighborhoods_all = neighborhoods\n",
    "        else: neighborhoods_all = np.r_[neighborhoods_all, neighborhoods]\n",
    "    return neighborhoods_all\n",
    "\n",
    "\n",
    "\n",
    "# ###### Show the frequency of labels to make threshold\n",
    "'''\n",
    "# #How to select top w elements of V according to labeling  \n",
    "df_node_label.cano_label.value_counts().plot(kind='bar')\n",
    "df_node_label.cano_label.value_counts().sort_index().plot(kind='bar',  figsize=(14,5))\n",
    "plt.title('Number of nodes by labeling')\n",
    "plt.xlabel('Labeling')\n",
    "plt.ylabel('Number of nodes')\n",
    "\n",
    "_SUM_ALL_NODES = df_node_label.shape[0]\n",
    "plt.twinx()\n",
    "plt.ylabel(\"Cummlative Sum Rate\", color=\"r\")\n",
    "plt.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "plt.plot(df_node_label.cano_label.value_counts().sort_index().index, \n",
    "         df_node_label.cano_label.value_counts().sort_index().cumsum() /_SUM_ALL_NODES, \"r-\", linewidth=2)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "# ### Get several nodes with a condition of cano_label (sequence)\n",
    "\n",
    "def tensor(neighborhoods, WIDTH_W, RECEPTIVE_FIELD_SIZE_K):\n",
    "    nodes_features = pd.get_dummies(df_node_label.label,\n",
    "                               columns=feature_list,\n",
    "                               sparse=True )\n",
    "\n",
    "    #### Reindex and transporse to get columns of get dummy #########\n",
    "    nodes_features = nodes_features.T.reindex(feature_list).T.fillna(0)\n",
    "    nodes_features = nodes_features.values\n",
    "    zero_features_for_padding_at_the_end = np.zeros((1, nodes_features.shape[1]), dtype=float)\n",
    "    nodes_features = np.r_[nodes_features, zero_features_for_padding_at_the_end]\n",
    "\n",
    "    i_list = np.reshape(neighborhoods, [-1])\n",
    "    the_place_of_zero_features_for_padding = i_list.max() + 1 \n",
    "    i_list = np.where(i_list<0 , the_place_of_zero_features_for_padding, i_list)\n",
    "    ret = nodes_features[i_list]\n",
    "\n",
    "    graph_data_size = len(df_node_label.graph_ind.unique())\n",
    "\n",
    "    ret_by_graph = np.reshape(ret, (graph_data_size, WIDTH_W*RECEPTIVE_FIELD_SIZE_K, num_features))\n",
    "    finalshape = ([graph_data_size, WIDTH_W, RECEPTIVE_FIELD_SIZE_K,num_features])\n",
    "    ret = np.reshape(ret_by_graph, finalshape)\n",
    "    return ret\n",
    "\n",
    "\n",
    "#NUM_NODES\n",
    "#RECEPTIVE_FIELD_SIZE_K = 20 # Receptive Field Size K\n",
    "#WIDTH_W = 8 #threshold based on canonical label\n",
    "\n",
    "feature_list = df_node_label['label'].unique()\n",
    "num_features = len(feature_list)\n",
    "adj_dict_by_graphId = create_adj_dict_by_graphId(df_adj, df_node_label)\n",
    "adj_coomatrix_by_graphId = create_adj_coomatrix_by_graphId(adj_dict_by_graphId, df_node_label)\n",
    "\n",
    "\n",
    "cano_label = canonical_labeling(adj_dict_by_graphId, df_node_label, df_adj)\n",
    "df_node_label = pd.concat([df_node_label, pd.Series(cano_label, dtype=int, name='cano_label')],  axis=1)\n",
    "\n",
    "\n",
    "def main(WIDTH_W, RECEPTIVE_FIELD_SIZE_K):\n",
    "    neighborhoods_graph = make_neighbor(adj_coomatrix_by_graphId, df_node_label, WIDTH_W=WIDTH_W, RECEPTIVE_FIELD_SIZE_K=RECEPTIVE_FIELD_SIZE_K)\n",
    "    result_tensor = tensor(neighborhoods_graph, WIDTH_W, RECEPTIVE_FIELD_SIZE_K)\n",
    "    return  result_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2 after neibor assemble:  14.727579832077026\n",
      "6 3 after neibor assemble:  14.975040912628174\n",
      "6 4 after neibor assemble:  15.027153015136719\n",
      "6 5 after neibor assemble:  15.468482971191406\n",
      "6 8 after neibor assemble:  14.019802808761597\n",
      "6 10 after neibor assemble:  15.321429967880249\n",
      "6 12 after neibor assemble:  14.36699104309082\n",
      "6 15 after neibor assemble:  13.378386974334717\n",
      "9 2 after neibor assemble:  14.128776788711548\n",
      "9 3 after neibor assemble:  14.914149761199951\n",
      "9 4 after neibor assemble:  15.643851041793823\n",
      "9 5 after neibor assemble:  16.204430103302002\n",
      "9 8 after neibor assemble:  16.24001908302307\n",
      "9 10 after neibor assemble:  16.123213052749634\n",
      "9 12 after neibor assemble:  16.389490842819214\n",
      "9 15 after neibor assemble:  15.852940082550049\n",
      "12 2 after neibor assemble:  17.754552125930786\n",
      "12 3 after neibor assemble:  17.22513222694397\n",
      "12 4 after neibor assemble:  16.90054488182068\n",
      "12 5 after neibor assemble:  16.20630407333374\n",
      "12 8 after neibor assemble:  18.320008993148804\n",
      "12 10 after neibor assemble:  17.336541891098022\n",
      "12 12 after neibor assemble:  16.754285097122192\n",
      "12 15 after neibor assemble:  16.50884699821472\n",
      "15 2 after neibor assemble:  18.501611948013306\n",
      "15 3 after neibor assemble:  18.056910753250122\n",
      "15 4 after neibor assemble:  17.206959009170532\n",
      "15 5 after neibor assemble:  18.655675172805786\n",
      "15 8 after neibor assemble:  18.803007125854492\n",
      "15 10 after neibor assemble:  18.821857929229736\n",
      "15 12 after neibor assemble:  20.198769092559814\n",
      "15 15 after neibor assemble:  18.859355211257935\n",
      "18 2 after neibor assemble:  18.16235899925232\n",
      "18 3 after neibor assemble:  18.743357181549072\n",
      "18 4 after neibor assemble:  19.37009310722351\n",
      "18 5 after neibor assemble:  19.497080087661743\n",
      "18 8 after neibor assemble:  19.58940625190735\n",
      "18 10 after neibor assemble:  20.771843671798706\n",
      "18 12 after neibor assemble:  18.27107000350952\n",
      "18 15 after neibor assemble:  20.891640186309814\n",
      "21 2 after neibor assemble:  19.38201069831848\n",
      "21 3 after neibor assemble:  18.840271949768066\n",
      "21 4 after neibor assemble:  20.091747999191284\n",
      "21 5 after neibor assemble:  19.628883123397827\n",
      "21 8 after neibor assemble:  18.21956515312195\n",
      "21 10 after neibor assemble:  18.64764904975891\n",
      "21 12 after neibor assemble:  19.86469602584839\n",
      "21 15 after neibor assemble:  22.049668788909912\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "list_WIDTH_W=[6, 9,12,15,18,21]\n",
    "list_RECEPTIVE_FIELD_SIZE_K=[2,3,4,5,8,10,12,15]\n",
    "#list_WIDTH_W=[15]\n",
    "#list_RECEPTIVE_FIELD_SIZE_K=[2]\n",
    "secondsret = pd.DataFrame(np.zeros((len(list_WIDTH_W),len(list_RECEPTIVE_FIELD_SIZE_K)),dtype=float))\n",
    "\n",
    "for w_ind, w in enumerate(list_WIDTH_W):\n",
    "    for k_ind, k in enumerate(list_RECEPTIVE_FIELD_SIZE_K):\n",
    "        now = time.time()        \n",
    "        result_patcy = main(w,k)\n",
    "        seconds = time.time() - now\n",
    "        print (w,k,'after neibor assemble: ', time.time() - now)\n",
    "        secondsret.iloc[w_ind, k_ind] = seconds\n",
    "        secondsret.columns = ['k' + str(k) for k in list_RECEPTIVE_FIELD_SIZE_K]\n",
    "        secondsret.index = ['w' + str(w) for w in list_WIDTH_W]\n",
    "        secondsret.to_csv('mutag_seconds_result2.csv')\n",
    "        #save_object(result_patcy, 'patchy_np_array_mutag_w{}k{}.pkl'.format(w,k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
