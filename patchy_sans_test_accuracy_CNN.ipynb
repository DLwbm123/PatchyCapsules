{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#import pydevd\n",
    "#pydevd.settrace('localhost', port=49309, stdoutToServer=True, stderrToServer=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, defaultdict\n",
    "from six.moves import xrange\n",
    "#from __pynauty__ import graph\n",
    "#import pynauty.graph\n",
    "import pynauty\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pynauty as nauty\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from PatchyTools import Dataset\n",
    "from PatchyTools import utils\n",
    "from PatchyTools import Patchy_san"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutag = Dataset.Dropbox('MUTAG')\n",
    "df_edge_label = mutag.get_edge_label()\n",
    "df_graph_ind = mutag.get_graph_ind()\n",
    "df_adj = mutag.get_adj()\n",
    "df_node_label = mutag.get_node_label()\n",
    "df_node_label = pd.concat([df_node_label, df_graph_ind.graph_ind], axis=1)\n",
    "del df_graph_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(df_node_label.label.unique())\n",
    "data_in_patchysan = Patchy_san.main(WIDTH_W=18, RECEPTIVE_FIELD_SIZE_K=10, datasetname='MUTAG')\n",
    "data_in_patchysan.shape[3]\n",
    "final_labels = mutag.get_graph_label().graph_label.values\n",
    "final_labels = pd.get_dummies(final_labels).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_FIELD_SIZE=10\n",
    "early_stopping = 60\n",
    "STOPPING_LOOKBACK_NUM =20\n",
    "test_range =10\n",
    "LEARNING_RATE = 0.0005\n",
    "EPSILON=1e-08\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################\n",
    "# Build a Multilayer Convolutional Network\n",
    "###########################################\n",
    "\n",
    "from __future__ import absolute_import, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "FeatureNum = data_in_patchysan.shape[3]\n",
    "PixcelSize = data_in_patchysan.shape[1]* data_in_patchysan.shape[2]\n",
    "\n",
    "W = tf.Variable(tf.zeros([PixcelSize, FeatureNum]))\n",
    "b = tf.Variable(tf.zeros([FeatureNum]))\n",
    "\n",
    "\"\"\"\n",
    "Weight Initialization\n",
    "\n",
    "To create this model, we're going to need to create a lot of weights and biases.\n",
    "One should generally initialize weights with a small amount of noise for symmetry breaking,\n",
    "and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize\n",
    "them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly\n",
    "while we build the model, let's create two handy functions to do it for us.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\"\"\"\n",
    "Convolution and Pooling\n",
    "\n",
    "TensorFlow also gives us a lot of flexibility in convolution and pooling operations.\n",
    "How do we handle the boundaries? What is our stride size? In this example,\n",
    "we're always going to choose the vanilla version. Our convolutions uses a stride of one\n",
    "and are zero padded so that the output is the same size as the input. Our pooling is plain old\n",
    "max pooling over 2x2 blocks. To keep our code cleaner, let's also abstract those operations into functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    #sprint('x.shape, W.shape', x.shape, W.shape)\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n",
    "                          strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "x_image = tf.placeholder(\"float\", shape=[None, 18, 10, 7]) #tf.reshape(data_in_patchysan, [-1, 18, 5, 7])\n",
    "y_ = tf.placeholder(\"float\", shape=[None,2])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1st layer\n",
    "patch size : 10x10 \n",
    "input channel : F\n",
    "output channel : 16\n",
    "\"\"\"\n",
    "\n",
    "W_conv1 = weight_variable([CONV_FIELD_SIZE,CONV_FIELD_SIZE,\n",
    "                           FeatureNum, \n",
    "                           16])\n",
    "\n",
    "b_conv1 = bias_variable([16])\n",
    "\n",
    "temp = conv2d(x_image, W_conv1) + b_conv1\n",
    "h_conv1 = tf.nn.relu(temp)\n",
    "\n",
    "\"\"\"\n",
    "1st layer\n",
    "patch size : 5x5\n",
    "input channel : 8\n",
    "output channel : 8\n",
    "\"\"\"\n",
    "\n",
    "W_conv2 = weight_variable([CONV_FIELD_SIZE, CONV_FIELD_SIZE, 16, 8])\n",
    "b_conv2 = bias_variable([8])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "#h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Densely Connected Layer\n",
    "\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow\n",
    "processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors,\n",
    "multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "\"\"\"\n",
    "\n",
    "#num = np.product([i for i in h_conv2.shape[1:]])\n",
    "num = 8 * data_in_patchysan.shape[1] *  data_in_patchysan.shape[2]\n",
    "h_conv2_flat = tf.reshape(h_conv2, [-1, num])\n",
    "\n",
    "W_fc1 = weight_variable([num, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\"\"\"\n",
    "Dropout\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder\n",
    "for the probability that a neuron's output is kept during dropout. This allows us to turn dropout\n",
    "on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically\n",
    "handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.\n",
    "\"\"\"\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\"\"\"\n",
    "Readout Layer\n",
    "Finally, we add a softmax layer, just like for the one layer softmax regression above.\n",
    "\"\"\"\n",
    "num_final_class = final_labels.shape[1]\n",
    "W_fc2 = weight_variable([128, num_final_class])\n",
    "b_fc2 = bias_variable([num_final_class])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(18), Dimension(10), Dimension(8)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training and evaluation \n",
    "TensorFlow\n",
    "\"\"\"\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, \n",
    "                                    epsilon=EPSILON, use_locking=False,\n",
    "                                    name='Adam'\n",
    "                                   ).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  #accuracy = tf.metrics.accuracy(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/koyamakyouhei/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, train: 0.64 , valid: 0.736842 \n",
      "step 20, train: 0.973333 , valid: 0.789474 \n",
      "step 40, train: 1 , valid: 0.789474 \n",
      "step 60, train: 1 , valid: 0.789474 \n",
      "Early Stopping with step at 61\n",
      "test accuracy 0.894737\n",
      "step 0, train: 0.666667 , valid: 0.526316 \n",
      "step 20, train: 0.953333 , valid: 0.631579 \n",
      "step 40, train: 1 , valid: 0.631579 \n",
      "step 60, train: 1 , valid: 0.631579 \n",
      "Early Stopping with step at 61\n",
      "test accuracy 0.789474\n",
      "step 0, train: 0.713333 , valid: 0.684211 \n",
      "step 20, train: 0.946667 , valid: 0.789474 \n",
      "step 40, train: 1 , valid: 0.736842 \n",
      "step 60, train: 1 , valid: 0.789474 \n",
      "Early Stopping with step at 61\n",
      "test accuracy 0.894737\n",
      "step 0, train: 0.36 , valid: 0.210526 \n",
      "step 20, train: 0.926667 , valid: 0.842105 \n",
      "step 40, train: 1 , valid: 0.842105 \n",
      "step 60, train: 1 , valid: 0.894737 \n",
      "Early Stopping with step at 61\n",
      "test accuracy 0.894737\n",
      "step 0, train: 0.64 , valid: 0.736842 \n",
      "step 20, train: 0.953333 , valid: 0.842105 \n",
      "step 40, train: 1 , valid: 0.842105 \n",
      "step 60, train: 1 , valid: 0.894737 \n",
      "Early Stopping with step at 61\n",
      "test accuracy 0.789474\n",
      "step 0, train: 0.366667 , valid: 0.368421 \n",
      "step 20, train: 0.946667 , valid: 0.947368 \n",
      "step 40, train: 0.993333 , valid: 0.842105 \n",
      "step 60, train: 1 , valid: 0.789474 \n",
      "Early Stopping with step at 64\n",
      "test accuracy 0.894737\n",
      "step 0, train: 0.34 , valid: 0.263158 \n",
      "step 20, train: 0.886667 , valid: 0.947368 \n",
      "step 40, train: 0.946667 , valid: 0.947368 \n",
      "step 60, train: 0.986667 , valid: 0.947368 \n",
      "step 80, train: 0.993333 , valid: 0.947368 \n",
      "step 100, train: 1 , valid: 0.947368 \n",
      "Early Stopping with step at 116\n",
      "test accuracy 0.842105\n",
      "step 0, train: 0.7 , valid: 0.631579 \n",
      "step 20, train: 0.94 , valid: 0.894737 \n",
      "step 40, train: 1 , valid: 0.894737 \n",
      "step 60, train: 1 , valid: 0.894737 \n",
      "Early Stopping with step at 62\n",
      "test accuracy 0.894737\n",
      "step 0, train: 0.626667 , valid: 0.789474 \n",
      "step 20, train: 0.946667 , valid: 0.894737 \n",
      "step 40, train: 1 , valid: 0.947368 \n",
      "step 60, train: 1 , valid: 0.894737 \n",
      "Early Stopping with step at 64\n",
      "test accuracy 0.789474\n",
      "step 0, train: 0.3 , valid: 0.526316 \n",
      "step 20, train: 0.9 , valid: 0.894737 \n",
      "step 40, train: 0.966667 , valid: 0.947368 \n",
      "step 60, train: 0.986667 , valid: 0.947368 \n",
      "step 80, train: 1 , valid: 0.947368 \n",
      "step 100, train: 1 , valid: 0.947368 \n",
      "test accuracy 0.842105\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost_val = []\n",
    "test_accuracy_list=[]\n",
    "\n",
    "\n",
    "'''\n",
    "Preapre k-folds index\n",
    "'''\n",
    "rs = ShuffleSplit(n_splits=test_range, random_state=0)\n",
    "rs.get_n_splits(data_in_patchysan)\n",
    "k_folds ={}\n",
    "kf=0\n",
    "for train_index, test_index in rs.split(data_in_patchysan):\n",
    "    k_folds[kf] =[train_index, test_index]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    kf+=1\n",
    "\n",
    "'''\n",
    "Conduct experiments\n",
    "'''\n",
    "\n",
    "for t in range(test_range):    \n",
    "    #seed = t\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    tf.global_variables_initializer()\n",
    "    \n",
    "    test_index = k_folds[t][1]    \n",
    "    train_index = k_folds[t][0]\n",
    "    valid_index = k_folds[t][0][:19]\n",
    "    train_index = k_folds[t][0][19:]\n",
    "    \n",
    "    X_train = data_in_patchysan[train_index]\n",
    "    y_train = final_labels[train_index]    \n",
    "    X_valid = data_in_patchysan[valid_index]\n",
    "    y_valid = final_labels[valid_index]\n",
    "    X_test = data_in_patchysan[test_index]\n",
    "    y_test = final_labels[test_index]\n",
    "    \n",
    "    for i in range(120):    \n",
    "        if i > early_stopping and cost_val[-1] > np.mean(cost_val[-STOPPING_LOOKBACK_NUM-1:-1]):\n",
    "            test_accuracy = accuracy.eval(feed_dict={x_image: X_test, y_: y_test, keep_prob: 1.0})\n",
    "            print ('Early Stopping with step at {}'.format(i, cost, test_accuracy) )\n",
    "            break\n",
    "        traincost = accuracy.eval(feed_dict={ x_image: X_train, y_: y_train,keep_prob: 1.0})\n",
    "        cost = accuracy.eval(feed_dict={x_image: X_valid, y_: y_valid,keep_prob: 1.0})\n",
    "        cost_val.append(cost)\n",
    "        if i %20==0: \n",
    "            print (\"step %d, train: %g , valid: %s \" % (i, traincost, cost))    \n",
    "        \n",
    "        train_step.run(feed_dict={x_image: X_train, y_: y_train, keep_prob: 0.5})\n",
    "    \n",
    "    test_accuracy = accuracy.eval(feed_dict={x_image: X_test, y_: y_test, keep_prob: 1.0})\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    print( \"test accuracy %g\" % accuracy.eval(feed_dict={x_image: X_test, y_: y_test, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.85263157, '+-', 0.045883127)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_accuracy_list), '+-', np.std(test_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.train.AdamOptimizer(learning_rate=rate_use, beta1=beta_1, beta2=beta_2, epsilon=eta, \n",
    "#                       use_locking=False, name='Adam').minimize(objective_value, global_step=step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
