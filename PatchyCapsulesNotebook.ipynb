{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pynauty'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-965b4a7314f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#from __pynauty__ import graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#import pynauty.graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpynauty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pynauty'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import json\n",
    "#import pydevd\n",
    "#pydevd.settrace('localhost', port=49309, stdoutToServer=True, stderrToServer=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, defaultdict\n",
    "from six.moves import xrange\n",
    "#from __pynauty__ import graph\n",
    "#import pynauty.graph\n",
    "import pynauty\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pynauty as nauty\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "import os\n",
    "\n",
    "\n",
    "from PatchyTools import Dataset, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "#Node id is made to start from 0 due to nauty package requirement, even if it starts from 1 in the original\n",
    "#Graph id is starting from 1\n",
    "'''\n",
    "\n",
    "mutag = Dataset.Dropbox('MUTAG')\n",
    "df_edge_label = mutag.get_edge_label()\n",
    "df_graph_ind = mutag.get_graph_ind()\n",
    "df_adj = mutag.get_adj()\n",
    "\n",
    "df_node_label = mutag.get_node_label()\n",
    "df_node_label = pd.concat([df_node_label, df_graph_ind.graph_label], axis=1)\n",
    "\n",
    "del df_graph_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_subset_adj(df_adj, df_node_label,graph_label_num):\n",
    "    df_glabel = df_node_label[df_node_label.graph_label == graph_label_num ]\n",
    "    index_of_glabel = (df_adj['to'].isin(df_glabel.node) & df_adj['from'].isin(df_glabel.node))\n",
    "    return df_adj[index_of_glabel]\n",
    "\n",
    "def get_smallest_node_id_from_adj(df_adj):\n",
    "    return min(df_adj['to'].min(), df_adj['from'].min())\n",
    "\n",
    "\n",
    "def create_adj_dict_by_graphId(df_adj, df_node_label):\n",
    "    '''\n",
    "    input: df_node_label\n",
    "    return: {1: {0:[0,2,5]}} = {graphId: {nodeId:[node,node,node]}}\n",
    "    '''\n",
    "    adj_dict_by_graphId ={}\n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = get_subset_adj(df_adj, df_node_label, graph_label_num=l)        \n",
    "        smallest_node_id = get_smallest_node_id_from_adj(df_subset_adj)\n",
    "        df_subset_adj -= smallest_node_id\n",
    "        adj_dict_by_graphId[l] = df_subset_adj\n",
    "    return adj_dict_by_graphId\n",
    "\n",
    "\n",
    "def canonical_labeling(adj_dict_by_graphId, df_node_label, df_adj):\n",
    "    all_canonical_labels =[]\n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = adj_dict_by_graphId[l]\n",
    "        df_subset_nodes = df_node_label[df_node_label.graph_label==l]        \n",
    "        temp_graph_dict = utils.dfadj_to_dict(df_subset_adj)\n",
    "        nauty_graph = nauty.Graph(len(temp_graph_dict), adjacency_dict=temp_graph_dict)\n",
    "        canonical_labeling = nauty.canonical_labeling(nauty_graph)        \n",
    "        canonical_labeling = [df_subset_nodes.label.values[i] for i in canonical_labeling] ###\n",
    "        all_canonical_labels += canonical_labeling\n",
    "    return all_canonical_labels\n",
    "\n",
    "\n",
    "def create_adj_coomatrix_by_graphId(adj_dict_by_graphId, df_node_label):\n",
    "    \"\"\"\n",
    "    return: a coomatrix per graphId\n",
    "    \"\"\"\n",
    "    \n",
    "    adj_coomatrix_by_graphId ={}\n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = adj_dict_by_graphId[l]\n",
    "        df_subset_node_label = df_node_label[df_node_label.graph_label == l]\n",
    "        adjacency = coo_matrix(( np.ones(len(df_subset_adj)), \n",
    "                                (df_subset_adj.iloc[:,0].values, df_subset_adj.iloc[:,1].values) ), \n",
    "                                 shape=(len(df_subset_node_label), len(df_subset_node_label))\n",
    "                              )\n",
    "        adj_coomatrix_by_graphId[l]=adjacency\n",
    "    return adj_coomatrix_by_graphId\n",
    "\n",
    "def make_neighbor(adj_coomatrix_by_graphId, df_node_label, WIDTH_W):\n",
    "    \n",
    "    \"\"\"\n",
    "    return: a dictionary with the shape of {graphId:[matrix: node x neighbor]} \n",
    "    The size of 2D matrix is (Node number) x (RECEPTIVE_FIELD_SIZE_K). \n",
    "    \"\"\"\n",
    "    \n",
    "    neighborhoods_dict=dict()\n",
    "    \n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        adjacency = adj_coomatrix_by_graphId[l]\n",
    "        graph = nx.from_numpy_matrix(adjacency.todense())\n",
    "        \n",
    "        # Create the neighbors with -1 for neighbor assemble.\n",
    "        #After this, if the RECEPTIVE_FIELD_SIZE_K exceeds the number of WIDTH_W, then fill them with -1\n",
    "        neighborhoods = np.zeros((WIDTH_W, RECEPTIVE_FIELD_SIZE_K), dtype=np.int32)\n",
    "        neighborhoods.fill(-1) \n",
    "        \n",
    "        df_sequence = df_node_label[df_node_label.graph_label == l]\n",
    "        df_sequence = df_sequence.sort_values(by='cano_label')\n",
    "        smallest_node_id = df_sequence.node.min()\n",
    "                \n",
    "        # CUT GRAPH BY THRESHOLD of cano_label ''' Top width w elements of V according to labeling  '''\n",
    "        df_sequence = df_sequence.iloc[:WIDTH_W,:]\n",
    "        df_sequence['node'] = df_sequence.node.values  - smallest_node_id        \n",
    "        \n",
    "        for i, node in enumerate(df_sequence.node):\n",
    "            #shortest = nx.single_source_dijkstra_path_length(graph, node).items()\n",
    "            df_shortest = pd.DataFrame.from_dict(nx.single_source_dijkstra_path_length(graph, node),\n",
    "                                                 orient='index') #\n",
    "            df_shortest.columns =['distance'] #\n",
    "            df_shortest['node'] = df_shortest.index.values #\n",
    "            df_shortest = pd.merge(df_node_label, df_shortest, on='node', how='right') #\n",
    "            \n",
    "            # Sort by distance and then by cano_label            \n",
    "            df_shortest = df_shortest.sort_values(by=['distance','cano_label']) #\n",
    "            df_shortest = df_shortest.iloc[:RECEPTIVE_FIELD_SIZE_K,:] #\n",
    "            #shortest = sorted(shortest, key=lambda v: v[1])            \n",
    "            #shortest = shortest[:RECEPTIVE_FIELD_SIZE_K]\n",
    "            for j in range(0, min(RECEPTIVE_FIELD_SIZE_K, len(df_shortest))):\n",
    "                #neighborhoods[i][j] = shortest[j][0]\n",
    "                neighborhoods[i][j] = df_shortest['node'].values[j]\n",
    "                \n",
    "        neighborhoods_dict[l]= neighborhoods.copy()\n",
    "    return neighborhoods_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "now = time.time()\n",
    "\n",
    "#NUM_NODES \n",
    "LABEL_THRESHOLD = 2 #threshold of canonical label\n",
    "RECEPTIVE_FIELD_SIZE_K = 20 #''' Receptive Field Size'''\n",
    "WIDTH_W = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main (Timing starts here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_dict_by_graphId = create_adj_dict_by_graphId(df_adj, df_node_label)\n",
    "cano_label = canonical_labeling(adj_dict_by_graphId, df_node_label, df_adj)\n",
    "df_node_label = pd.concat([df_node_label, pd.Series(cano_label, dtype=int, name='cano_label')],  axis=1)\n",
    "\n",
    "#cert_list = [i for i in (nauty.certificate(nauty_graph))]\n",
    "# '''canonical_labeling = [df_node_label.label.values[i] for i in canonical_labeling]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Show the frequency of labels to make threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# #How to select top w elements of V according to labeling  \\ndf_node_label.cano_label.value_counts().plot(kind=\\'bar\\')\\ndf_node_label.cano_label.value_counts().sort_index().plot(kind=\\'bar\\',  figsize=(14,5))\\nplt.title(\\'Number of nodes by labeling\\')\\nplt.xlabel(\\'Labeling\\')\\nplt.ylabel(\\'Number of nodes\\')\\n\\n_SUM_ALL_NODES = df_node_label.shape[0]\\nplt.twinx()\\nplt.ylabel(\"Cummlative Sum Rate\", color=\"r\")\\nplt.tick_params(axis=\"y\", labelcolor=\"r\")\\nplt.plot(df_node_label.cano_label.value_counts().sort_index().index, \\n         df_node_label.cano_label.value_counts().sort_index().cumsum() /_SUM_ALL_NODES, \"r-\", linewidth=2)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# #How to select top w elements of V according to labeling  \n",
    "df_node_label.cano_label.value_counts().plot(kind='bar')\n",
    "df_node_label.cano_label.value_counts().sort_index().plot(kind='bar',  figsize=(14,5))\n",
    "plt.title('Number of nodes by labeling')\n",
    "plt.xlabel('Labeling')\n",
    "plt.ylabel('Number of nodes')\n",
    "\n",
    "_SUM_ALL_NODES = df_node_label.shape[0]\n",
    "plt.twinx()\n",
    "plt.ylabel(\"Cummlative Sum Rate\", color=\"r\")\n",
    "plt.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "plt.plot(df_node_label.cano_label.value_counts().sort_index().index, \n",
    "         df_node_label.cano_label.value_counts().sort_index().cumsum() /_SUM_ALL_NODES, \"r-\", linewidth=2)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get several nodes with a condition of cano_label (sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_coomatrix_by_graphId = create_adj_coomatrix_by_graphId(adj_dict_by_graphId, df_node_label)\n",
    "neighborhoods_graph = make_neighbor(adj_coomatrix_by_graphId, df_node_label, WIDTH_W=WIDTH_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things about tensorflow constraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nneighborhoods[graphId]: This represents the matrix of (nodes x neighbor).\\nnodes: This represents the matrix of (nodes x features).\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "neighborhoods[graphId]: This represents the matrix of (nodes x neighbor).\n",
    "nodes: This represents the matrix of (nodes x features).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = df_node_label['label'].unique()\n",
    "num_features = len(feature_list)\n",
    "\n",
    "def main_timing(graph_id):\n",
    "    neighborhoods = tf.constant(neighborhoods_graph[graph_id], dtype=tf.int32)\n",
    "    sparse_df = pd.get_dummies(df_node_label.loc[df_node_label.graph_label==graph_id].label, \n",
    "                               columns=feature_list,\n",
    "                               sparse=True\n",
    "                              )\n",
    "    \n",
    "    #### Reindex and transporse to get columns of get dummy #########\n",
    "    sparse_df = sparse_df.T.reindex(feature_list).T.fillna(0)\n",
    "    nodes = tf.constant(sparse_df.values, dtype=tf.int32 )\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        data = tf.reshape(neighborhoods, [-1])\n",
    "        i = tf.maximum(data, 0)\n",
    "        i_list = i.eval()\n",
    "        #print(i_list)\n",
    "        #print('')\n",
    "\n",
    "        for ind, i in enumerate(i_list):\n",
    "            if ind ==0: \n",
    "                positive = tf.strided_slice(nodes, [i], [i+1], [1])\n",
    "                #positive = tf.constant(temp)\n",
    "            else:\n",
    "                temp = tf.strided_slice(nodes, [i], [i+1], [1])\n",
    "                positive = tf.concat([positive,temp],axis=0)\n",
    "\n",
    "        negative = tf.zeros([positive.shape[0], positive.shape[1]], dtype= tf.int32)\n",
    "\n",
    "        #print ('shape', data.shape, negative.shape, positive.shape)\n",
    "        #print ( data, negative, positive)\n",
    "\n",
    "        # padding with 0 here because -1 indicates there is no neighbor nodes.\n",
    "        ret = tf.where(data < 0, negative, positive)\n",
    "        #print('padding done')\n",
    "\n",
    "\n",
    "        ret = tf.reshape(ret, \n",
    "                         [neighborhoods.shape[0], \n",
    "                          RECEPTIVE_FIELD_SIZE_K, # This is equals to neighborhoods.shape[1]\n",
    "                          num_features])\n",
    "        #print (key, 'ret.shape: ',ret.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 29/188 [00:23<02:07,  1.24it/s]"
     ]
    }
   ],
   "source": [
    "#stop stp stp\n",
    "for key in tqdm(adj_dict_by_graphId.keys()):\n",
    "    main_timing(key)\n",
    "\n",
    "print ('time passed in seconds', (\"%.2f\"%(time.time() - now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
