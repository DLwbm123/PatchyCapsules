{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, defaultdict\n",
    "from six.moves import xrange\n",
    "\n",
    "import pynauty as nauty\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import coo_matrix\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('PatchyTools')\n",
    "sys.path.append('../PatchyCapsules/')\n",
    "\n",
    "from GraphConverter import GraphConverter\n",
    "from DropboxLoader import DropboxLoader\n",
    "\n",
    "from keras_capsule_net import CapsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG dataset already exists\n"
     ]
    }
   ],
   "source": [
    "# Getting the training data:\n",
    "dataset_name = 'MUTAG'\n",
    "width = 18\n",
    "receptive_field = 10\n",
    "PatchyConverter = GraphConverter(dataset_name, width, receptive_field)\n",
    "mutag_tensor = PatchyConverter.graphs_to_Patchy_tensor()\n",
    "#plt.imshow(mutag_tensor[0,:,:,2])\n",
    "\n",
    "# Getting the labels:\n",
    "dropbox_loader = DropboxLoader(dataset_name)\n",
    "mutag_labels = dropbox_loader.get_graph_label()\n",
    "mutag_labels = np.array(mutag_labels.graph_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = mutag_tensor.shape[1:]\n",
    "model, eval_model, manipulate_model = CapsNet(input_shape=input_shape,\n",
    "                                              n_class=len(np.unique(mutag_labels)),\n",
    "                                              routings=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras_capsule_net import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mutag_tensor, mutag_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers.core import Dropout, Reshape\n",
    "\n",
    "def test_layer(layer, x):\n",
    "\n",
    "    layer_config = layer.get_config()\n",
    "    print(layer_config)\n",
    "    layer_config[\"input_shape\"] = x.shape\n",
    "    layer = layer.__class__.from_config(layer_config)\n",
    "    print(layer)\n",
    "    model = Sequential()\n",
    "    model.add(layer)\n",
    "    model.compile(\"rmsprop\", \"mse\")\n",
    "\n",
    "    x_ = np.expand_dims(x, axis=0)\n",
    "    return model.predict(x_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dropout_5', 'trainable': True, 'rate': 0.5, 'noise_shape': None, 'seed': None}\n",
      "<keras.layers.core.Dropout object at 0x126ab9080>\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dropout, Reshape\n",
    "\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(10, 10)\n",
    "layer = Dropout(0.5)\n",
    "y = test_layer(layer, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(df_node_label.label.unique())\n",
    "print('number of features : {}'.format(num_features))\n",
    "data_in_patchysan = Patchy_san.main(WIDTH_W=18, RECEPTIVE_FIELD_SIZE_K=10, datasetname='MUTAG')\n",
    "data_in_patchysan.shape[3]\n",
    "#final_labels = mutag.get_graph_label().graph_label.values\n",
    "#final_labels = pd.get_dummies(final_labels).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'MUTAG'\n",
    "GAMMA_ENV = os.environ['GAMMA_DATA_ROOT']\n",
    "root_gamma_path = GAMMA_ENV+'Samples'\n",
    "node_label_filename =  '{0}/{0}_node_labels.txt'.format(dataset_name)\n",
    "node_label_path = os.path.join(root_gamma_path, node_label_filename)\n",
    "pd.read_csv(node_label_path , delimiter=' ',header=None,index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_subset_adj(df_adj, df_node_label,graph_label_num):\n",
    "    df_glabel = df_node_label[df_node_label.graph_label == graph_label_num ]\n",
    "    index_of_glabel = (df_adj['to'].isin(df_glabel.node) & df_adj['from'].isin(df_glabel.node))\n",
    "    return df_adj[index_of_glabel]\n",
    "\n",
    "def get_smallest_node_id_from_adj(df_adj):\n",
    "    return min(df_adj['to'].min(), df_adj['from'].min())\n",
    "\n",
    "\n",
    "def create_adj_dict_by_graphId(df_adj, df_node_label):\n",
    "    '''\n",
    "    input: df_node_label\n",
    "    return: {1: {0:[0,2,5]}} = {graphId: {nodeId:[node,node,node]}}\n",
    "    '''\n",
    "    adj_dict_by_graphId ={}\n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = get_subset_adj(df_adj, df_node_label, graph_label_num=l)        \n",
    "        smallest_node_id = get_smallest_node_id_from_adj(df_subset_adj)\n",
    "        df_subset_adj -= smallest_node_id\n",
    "        adj_dict_by_graphId[l] = df_subset_adj\n",
    "    return adj_dict_by_graphId\n",
    "\n",
    "\n",
    "def canonical_labeling(adj_dict_by_graphId, df_node_label, df_adj):\n",
    "    all_canonical_labels =[]\n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = adj_dict_by_graphId[l]\n",
    "        df_subset_nodes = df_node_label[df_node_label.graph_label==l]        \n",
    "        temp_graph_dict = utils.dfadj_to_dict(df_subset_adj)\n",
    "        nauty_graph = nauty.Graph(len(temp_graph_dict), adjacency_dict=temp_graph_dict)\n",
    "        canonical_labeling = nauty.canonical_labeling(nauty_graph)        \n",
    "        canonical_labeling = [df_subset_nodes.label.values[i] for i in canonical_labeling] ###\n",
    "        all_canonical_labels += canonical_labeling\n",
    "    return all_canonical_labels\n",
    "\n",
    "\n",
    "def create_adj_coomatrix_by_graphId(adj_dict_by_graphId, df_node_label):\n",
    "    \"\"\"\n",
    "    return: a coomatrix per graphId\n",
    "    \"\"\"\n",
    "    \n",
    "    adj_coomatrix_by_graphId ={}\n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        df_subset_adj = adj_dict_by_graphId[l]\n",
    "        df_subset_node_label = df_node_label[df_node_label.graph_label == l]\n",
    "        adjacency = coo_matrix(( np.ones(len(df_subset_adj)), \n",
    "                                (df_subset_adj.iloc[:,0].values, df_subset_adj.iloc[:,1].values) ), \n",
    "                                 shape=(len(df_subset_node_label), len(df_subset_node_label))\n",
    "                              )\n",
    "        adj_coomatrix_by_graphId[l]=adjacency\n",
    "    return adj_coomatrix_by_graphId\n",
    "\n",
    "def make_neighbor(adj_coomatrix_by_graphId, df_node_label, WIDTH_W):\n",
    "    \n",
    "    \"\"\"\n",
    "    return: a dictionary with the shape of {graphId:[matrix: node x neighbor]} \n",
    "    The size of 2D matrix is (Node number) x (RECEPTIVE_FIELD_SIZE_K). \n",
    "    \"\"\"\n",
    "    \n",
    "    neighborhoods_dict=dict()\n",
    "    \n",
    "    unique_graph_labels = df_node_label.graph_label.unique()\n",
    "    for l in unique_graph_labels:\n",
    "        adjacency = adj_coomatrix_by_graphId[l]\n",
    "        graph = nx.from_numpy_matrix(adjacency.todense())\n",
    "        \n",
    "        # Create the neighbors with -1 for neighbor assemble.\n",
    "        #After this, if the RECEPTIVE_FIELD_SIZE_K exceeds the number of WIDTH_W, then fill them with -1\n",
    "        neighborhoods = np.zeros((WIDTH_W, RECEPTIVE_FIELD_SIZE_K), dtype=np.int32)\n",
    "        neighborhoods.fill(-1) \n",
    "        \n",
    "        df_sequence = df_node_label[df_node_label.graph_label == l]\n",
    "        df_sequence = df_sequence.sort_values(by='cano_label')\n",
    "        smallest_node_id = df_sequence.node.min()\n",
    "                \n",
    "        # CUT GRAPH BY THRESHOLD of cano_label ''' Top width w elements of V according to labeling  '''\n",
    "        df_sequence = df_sequence.iloc[:WIDTH_W,:]\n",
    "        df_sequence['node'] = df_sequence.node.values  - smallest_node_id        \n",
    "        \n",
    "        for i, node in enumerate(df_sequence.node):\n",
    "            #shortest = nx.single_source_dijkstra_path_length(graph, node).items()\n",
    "            df_shortest = pd.DataFrame.from_dict(nx.single_source_dijkstra_path_length(graph, node),\n",
    "                                                 orient='index') #\n",
    "            df_shortest.columns =['distance'] #\n",
    "            df_shortest['node'] = df_shortest.index.values #\n",
    "            df_shortest = pd.merge(df_node_label, df_shortest, on='node', how='right') #\n",
    "            \n",
    "            # Sort by distance and then by cano_label            \n",
    "            df_shortest = df_shortest.sort_values(by=['distance','cano_label']) #\n",
    "            df_shortest = df_shortest.iloc[:RECEPTIVE_FIELD_SIZE_K,:] #\n",
    "            #shortest = sorted(shortest, key=lambda v: v[1])            \n",
    "            #shortest = shortest[:RECEPTIVE_FIELD_SIZE_K]\n",
    "            for j in range(0, min(RECEPTIVE_FIELD_SIZE_K, len(df_shortest))):\n",
    "                #neighborhoods[i][j] = shortest[j][0]\n",
    "                neighborhoods[i][j] = df_shortest['node'].values[j]\n",
    "                \n",
    "        neighborhoods_dict[l]= neighborhoods.copy()\n",
    "    return neighborhoods_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "now = time.time()\n",
    "\n",
    "#NUM_NODES \n",
    "LABEL_THRESHOLD = 2 #threshold of canonical label\n",
    "RECEPTIVE_FIELD_SIZE_K = 20 #''' Receptive Field Size'''\n",
    "WIDTH_W = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main (Timing starts here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_dict_by_graphId = create_adj_dict_by_graphId(df_adj, df_node_label)\n",
    "cano_label = canonical_labeling(adj_dict_by_graphId, df_node_label, df_adj)\n",
    "df_node_label = pd.concat([df_node_label, pd.Series(cano_label, dtype=int, name='cano_label')],  axis=1)\n",
    "\n",
    "#cert_list = [i for i in (nauty.certificate(nauty_graph))]\n",
    "# '''canonical_labeling = [df_node_label.label.values[i] for i in canonical_labeling]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Show the frequency of labels to make threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# #How to select top w elements of V according to labeling  \n",
    "df_node_label.cano_label.value_counts().plot(kind='bar')\n",
    "df_node_label.cano_label.value_counts().sort_index().plot(kind='bar',  figsize=(14,5))\n",
    "plt.title('Number of nodes by labeling')\n",
    "plt.xlabel('Labeling')\n",
    "plt.ylabel('Number of nodes')\n",
    "\n",
    "_SUM_ALL_NODES = df_node_label.shape[0]\n",
    "plt.twinx()\n",
    "plt.ylabel(\"Cummlative Sum Rate\", color=\"r\")\n",
    "plt.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "plt.plot(df_node_label.cano_label.value_counts().sort_index().index, \n",
    "         df_node_label.cano_label.value_counts().sort_index().cumsum() /_SUM_ALL_NODES, \"r-\", linewidth=2)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get several nodes with a condition of cano_label (sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_coomatrix_by_graphId = create_adj_coomatrix_by_graphId(adj_dict_by_graphId, df_node_label)\n",
    "neighborhoods_graph = make_neighbor(adj_coomatrix_by_graphId, df_node_label, WIDTH_W=WIDTH_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things about tensorflow constraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neighborhoods[graphId]: This represents the matrix of (nodes x neighbor).\n",
    "nodes: This represents the matrix of (nodes x features).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = df_node_label['label'].unique()\n",
    "num_features = len(feature_list)\n",
    "\n",
    "def main_timing(graph_id):\n",
    "    neighborhoods = tf.constant(neighborhoods_graph[graph_id], dtype=tf.int32)\n",
    "    sparse_df = pd.get_dummies(df_node_label.loc[df_node_label.graph_label==graph_id].label, \n",
    "                               columns=feature_list,\n",
    "                               sparse=True\n",
    "                              )\n",
    "    \n",
    "    #### Reindex and transporse to get columns of get dummy #########\n",
    "    sparse_df = sparse_df.T.reindex(feature_list).T.fillna(0)\n",
    "    nodes = tf.constant(sparse_df.values, dtype=tf.int32 )\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        data = tf.reshape(neighborhoods, [-1])\n",
    "        i = tf.maximum(data, 0)\n",
    "        i_list = i.eval()\n",
    "        #print(i_list)\n",
    "        #print('')\n",
    "\n",
    "        for ind, i in enumerate(i_list):\n",
    "            if ind ==0: \n",
    "                positive = tf.strided_slice(nodes, [i], [i+1], [1])\n",
    "                #positive = tf.constant(temp)\n",
    "            else:\n",
    "                temp = tf.strided_slice(nodes, [i], [i+1], [1])\n",
    "                positive = tf.concat([positive,temp],axis=0)\n",
    "\n",
    "        negative = tf.zeros([positive.shape[0], positive.shape[1]], dtype= tf.int32)\n",
    "\n",
    "        #print ('shape', data.shape, negative.shape, positive.shape)\n",
    "        #print ( data, negative, positive)\n",
    "\n",
    "        # padding with 0 here because -1 indicates there is no neighbor nodes.\n",
    "        ret = tf.where(data < 0, negative, positive)\n",
    "        #print('padding done')\n",
    "\n",
    "\n",
    "        ret = tf.reshape(ret, \n",
    "                         [neighborhoods.shape[0], \n",
    "                          RECEPTIVE_FIELD_SIZE_K, # This is equals to neighborhoods.shape[1]\n",
    "                          num_features])\n",
    "        #print (key, 'ret.shape: ',ret.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#stop stp stp\n",
    "for key in tqdm(adj_dict_by_graphId.keys()):\n",
    "    main_timing(key)\n",
    "\n",
    "print ('time passed in seconds', (\"%.2f\"%(time.time() - now)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
