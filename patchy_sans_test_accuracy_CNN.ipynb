{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#import pydevd\n",
    "#pydevd.settrace('localhost', port=49309, stdoutToServer=True, stderrToServer=True)\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, defaultdict\n",
    "from six.moves import xrange\n",
    "#from __pynauty__ import graph\n",
    "#import pynauty.graph\n",
    "import pynauty\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pynauty as nauty\n",
    "from multiprocessing import Pool\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "sys.path.append('PatchyTools')\n",
    "sys.path.append('../PatchyCapsules/')\n",
    "from DropboxLoader import DropboxLoader\n",
    "from GraphConverter import GraphConverter\n",
    "from PatchyTools import utils\n",
    "from PatchyTools import Patchy_san"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutag = Dataset.Dropbox('MUTAG')\n",
    "# df_edge_label = mutag.get_edge_label()\n",
    "# df_graph_ind = mutag.get_graph_ind()\n",
    "# df_adj = mutag.get_adj()\n",
    "# df_node_label = mutag.get_node_label()\n",
    "# df_node_label = pd.concat([df_node_label, df_graph_ind.graph_ind], axis=1)\n",
    "# del df_graph_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONV_FIELD_SIZE=10\n",
    "early_stopping = 60\n",
    "STOPPING_LOOKBACK_NUM =20\n",
    "test_range =10\n",
    "LEARNING_RATE = 0.0005\n",
    "EPSILON=1e-08\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG tensor exists, loading it from Dropbox\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Getting the data:\n",
    "dataset_name = 'MUTAG'\n",
    "num_classes = 2\n",
    "width = 18\n",
    "receptive_field = 10\n",
    "PatchyConverter = GraphConverter(dataset_name, width, receptive_field)\n",
    "mutag_tensor = PatchyConverter.graphs_to_Patchy_tensor()\n",
    "# plt.imshow(mutag_tensor[0,:,:,2])\n",
    "\n",
    "# Getting the labels:\n",
    "dropbox_loader = DropboxLoader(dataset_name)\n",
    "mutag_labels = dropbox_loader.get_graph_label()\n",
    "mutag_labels = np.array(mutag_labels.graph_label)\n",
    "\n",
    "# Assigning the data:\n",
    "x_train, x_test, y_train, y_test = train_test_split(mutag_tensor, mutag_labels, test_size=0.10)\n",
    "data = ((x_train,y_train),(x_test, y_test))\n",
    "input_shape = x_train.shape[1:]\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 18, 10, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in_patchysan = mutag_tensor\n",
    "final_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################\n",
    "# Build a Multilayer Convolutional Network\n",
    "###########################################\n",
    "\n",
    "from __future__ import absolute_import, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "FeatureNum = data_in_patchysan.shape[3]\n",
    "PixcelSize = data_in_patchysan.shape[1]* data_in_patchysan.shape[2]\n",
    "#FeatureNum = x_train.shape[3]\n",
    "#PixcelSize = x_train.shape[1]*x_train.shape[2]\n",
    "\n",
    "W = tf.Variable(tf.zeros([PixcelSize, FeatureNum]))\n",
    "b = tf.Variable(tf.zeros([FeatureNum]))\n",
    "\n",
    "\"\"\"\n",
    "Weight Initialization\n",
    "\n",
    "To create this model, we're going to need to create a lot of weights and biases.\n",
    "One should generally initialize weights with a small amount of noise for symmetry breaking,\n",
    "and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize\n",
    "them with a slightly positive initial bias to avoid \"dead neurons.\" Instead of doing this repeatedly\n",
    "while we build the model, let's create two handy functions to do it for us.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\"\"\"\n",
    "Convolution and Pooling\n",
    "\n",
    "TensorFlow also gives us a lot of flexibility in convolution and pooling operations.\n",
    "How do we handle the boundaries? What is our stride size? In this example,\n",
    "we're always going to choose the vanilla version. Our convolutions uses a stride of one\n",
    "and are zero padded so that the output is the same size as the input. Our pooling is plain old\n",
    "max pooling over 2x2 blocks. To keep our code cleaner, let's also abstract those operations into functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    #sprint('x.shape, W.shape', x.shape, W.shape)\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n",
    "                          strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "x_image = tf.placeholder(\"float\", shape=[None, 18, 10, 7]) #tf.reshape(data_in_patchysan, [-1, 18, 5, 7])\n",
    "y_ = tf.placeholder(\"float\", shape=[None,2])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1st layer\n",
    "patch size : 10x10 \n",
    "input channel : F\n",
    "output channel : 16\n",
    "\"\"\"\n",
    "\n",
    "W_conv1 = weight_variable([CONV_FIELD_SIZE,CONV_FIELD_SIZE,\n",
    "                           FeatureNum, \n",
    "                           16])\n",
    "\n",
    "b_conv1 = bias_variable([16])\n",
    "\n",
    "temp = conv2d(x_image, W_conv1) + b_conv1\n",
    "h_conv1 = tf.nn.relu(temp)\n",
    "\n",
    "\"\"\"\n",
    "1st layer\n",
    "patch size : 5x5\n",
    "input channel : 8\n",
    "output channel : 8\n",
    "\"\"\"\n",
    "\n",
    "W_conv2 = weight_variable([CONV_FIELD_SIZE, CONV_FIELD_SIZE, 16, 8])\n",
    "b_conv2 = bias_variable([8])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "#h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Densely Connected Layer\n",
    "\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow\n",
    "processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors,\n",
    "multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "\"\"\"\n",
    "\n",
    "#num = np.product([i for i in h_conv2.shape[1:]])\n",
    "num = 8 * data_in_patchysan.shape[1] *  data_in_patchysan.shape[2]\n",
    "#num = 8 * x_train.shape[1] *  x_train.shape[2]\n",
    "h_conv2_flat = tf.reshape(h_conv2, [-1, num])\n",
    "\n",
    "W_fc1 = weight_variable([num, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\"\"\"\n",
    "Dropout\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder\n",
    "for the probability that a neuron's output is kept during dropout. This allows us to turn dropout\n",
    "on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically\n",
    "handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling.\n",
    "\"\"\"\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\"\"\"\n",
    "Readout Layer\n",
    "Finally, we add a softmax layer, just like for the one layer softmax regression above.\n",
    "\"\"\"\n",
    "num_final_class = final_labels.shape[1]\n",
    "#num_final_class = y_train.shape[1]\n",
    "W_fc2 = weight_variable([128, num_final_class])\n",
    "b_fc2 = bias_variable([num_final_class])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training and evaluation \n",
    "TensorFlow\n",
    "\"\"\"\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, \n",
    "                                    epsilon=EPSILON, use_locking=False,\n",
    "                                    name='Adam'\n",
    "                                   ).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  #accuracy = tf.metrics.accuracy(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97, 139,  96, 107, 106, 151, 167,  55, 153,  26, 143, 156,  33,\n",
       "         8, 108, 122,  37,  61,  44])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 139 is out of bounds for axis 0 with size 133",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1b9398905f19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 139 is out of bounds for axis 0 with size 133"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "cost_val = []\n",
    "test_accuracy_list=[]\n",
    "\n",
    "\n",
    "'''\n",
    "Preapre k-folds index\n",
    "'''\n",
    "rs = ShuffleSplit(n_splits=test_range, random_state=0)\n",
    "#rs.get_n_splits(data_in_patchysan)\n",
    "rs.get_n_splits(x_train)\n",
    "k_folds ={}\n",
    "kf=0\n",
    "for train_index, test_index in rs.split(data_in_patchysan):\n",
    "#for train_index, test_index in rs.split(x_train):\n",
    "    k_folds[kf] =[train_index, test_index]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    kf+=1\n",
    "\n",
    "'''\n",
    "Conduct experiments\n",
    "'''\n",
    "\n",
    "for t in range(test_range):    \n",
    "    #seed = t\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    tf.global_variables_initializer()\n",
    "    \n",
    "    test_index = k_folds[t][1]    \n",
    "    train_index = k_folds[t][0]\n",
    "    valid_index = k_folds[t][0][:19]\n",
    "    train_index = k_folds[t][0][19:]\n",
    "    \n",
    "    X_train = data_in_patchysan[train_index]\n",
    "    y_train = final_labels[train_index]    \n",
    "    X_valid = data_in_patchysan[valid_index]\n",
    "    y_valid = final_labels[valid_index]\n",
    "    X_test = data_in_patchysan[test_index]\n",
    "    y_test = final_labels[test_index]\n",
    "\n",
    "\n",
    "#     X_train = x_train[train_index]\n",
    "#     y_train = y_train[train_index]    \n",
    "#     X_valid = x_train[valid_index]\n",
    "#     y_valid = y_train[valid_index]\n",
    "#     X_test = x_test\n",
    "#     y_test = y_test\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(120):    \n",
    "        if i > early_stopping and cost_val[-1] > np.mean(cost_val[-STOPPING_LOOKBACK_NUM-1:-1]):\n",
    "            test_accuracy = accuracy.eval(feed_dict={x_image: X_test, y_: y_test, keep_prob: 1.0})\n",
    "            print ('Early Stopping with step at {}'.format(i, cost, test_accuracy) )\n",
    "            break\n",
    "        traincost = accuracy.eval(feed_dict={ x_image: X_train, y_: y_train,keep_prob: 1.0})\n",
    "        cost = accuracy.eval(feed_dict={x_image: X_valid, y_: y_valid,keep_prob: 1.0})\n",
    "        cost_val.append(cost)\n",
    "        if i %20==0: \n",
    "            print (\"step %d, train: %g , valid: %s \" % (i, traincost, cost))    \n",
    "        \n",
    "        train_step.run(feed_dict={x_image: X_train, y_: y_train, keep_prob: 0.5})\n",
    "    \n",
    "    test_accuracy = accuracy.eval(feed_dict={x_image: X_test, y_: y_test, keep_prob: 1.0})\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    print( \"test accuracy %g\" % accuracy.eval(feed_dict={x_image: X_test, y_: y_test, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(test_accuracy_list), '+-', np.std(test_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.train.AdamOptimizer(learning_rate=rate_use, beta1=beta_1, beta2=beta_2, epsilon=eta, \n",
    "#                       use_locking=False, name='Adam').minimize(objective_value, global_step=step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
