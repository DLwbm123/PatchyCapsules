{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import copy\n",
    "\n",
    "from utils import combine_images\n",
    "from utils import plot_log\n",
    "\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "sys.path.append('./PatchyTools/')\n",
    "\n",
    "from PatchyConverter import PatchyConverter\n",
    "from DropboxLoader import DropboxLoader\n",
    "from CapsuleParameters import CapsuleParameters\n",
    "from CapsuleParameters import CapsuleTrainingParameters\n",
    "from GraphClassifier import GraphClassifier\n",
    "from ConvNetPatchy import ConvNetPatchy\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New fucntions:\n",
    "def print_nodes(nx_graph):\n",
    "    for i in nx_graph.nodes:\n",
    "        print(i,nx_graph.node[i])\n",
    "def print_dict(dictio):\n",
    "    for i in dictio.items():\n",
    "        print(i)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing graph data\n",
      "number of graphs in MUTAG dataset : 188\n",
      "number of features : 7\n",
      "Separating Graphs per graph ID\n",
      "getting width\n",
      "width: 18\n",
      "MUTAG tensor exists, loading it from Dropbox\n",
      "Loading path: /Users/marcelogutierrez/.gamma_link/Samples/MUTAG/MUTAG_patchy_tensor_w_18.npy\n"
     ]
    }
   ],
   "source": [
    "# Getting the data:\n",
    "dataset_name = 'MUTAG'\n",
    "width = 18\n",
    "receptive_field = 10\n",
    "s = 1\n",
    "relabeling = True\n",
    "\n",
    "# Get features:\n",
    "graph_converter = PatchyConverter(dataset_name,receptive_field,s)\n",
    "graph_tensor = graph_converter.graphs_to_Patchy_tensor()\n",
    "\n",
    "# Getting the labels:\n",
    "dropbox_loader = DropboxLoader(dataset_name)\n",
    "graph_labels = dropbox_loader.get_graph_label()\n",
    "graph_labels = np.array(graph_labels.graph_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time preprocess data in s 13.646012306213379\n"
     ]
    }
   ],
   "source": [
    "mutag_tensor = graph_converter.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutag_tensor2 = mutag_tensor.reshape(graph_converter.num_graphs,graph_converter.width, graph_converter.k,graph_converter.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 completed with val acc : 0.7894737124443054\n",
      "Fold 1 completed with val acc : 1.0\n",
      "Fold 2 completed with val acc : 0.7894737124443054\n",
      "Fold 3 completed with val acc : 0.7368420958518982\n",
      "Fold 4 completed with val acc : 0.9473684430122375\n",
      "Fold 5 completed with val acc : 0.8947368264198303\n",
      "Fold 6 completed with val acc : 0.9473684430122375\n",
      "Fold 7 completed with val acc : 0.8421052694320679\n",
      "Fold 8 completed with val acc : 0.7894737124443054\n",
      "Fold 9 completed with val acc : 0.9473684430122375\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "training_time = []\n",
    "n_folds = 10\n",
    "epochs = 400\n",
    "for j in range(n_folds):\n",
    "    # Starting Conv net with Patchy\n",
    "    patchy_cnn = ConvNetPatchy(mutag_tensor2, graph_labels)\n",
    "    patchy_cnn.split_data(random_state=j)\n",
    "    patchy_cnn.build_graph()\n",
    "    patchy_cnn.train_model(epochs=epochs)\n",
    "    print('Fold {} completed with val acc : {}'.format(j, patchy_cnn.final_val_acc))\n",
    "\n",
    "    val_acc.append(patchy_cnn.final_val_acc)\n",
    "    training_time.append(patchy_cnn.training_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(val_acc))\n",
    "print(np.std(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684210658073426"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to generate the model: 0.3308100700378418\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 1 trained \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to generate the model: 0.29631805419921875\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 2 trained \n",
      "time to generate the model: 0.28438615798950195\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 3 trained \n",
      "time to generate the model: 0.3378407955169678\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 4 trained \n",
      "time to generate the model: 0.3094780445098877\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 5 trained \n",
      "time to generate the model: 0.2848930358886719\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 6 trained \n",
      "time to generate the model: 0.27689290046691895\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 7 trained \n",
      "time to generate the model: 0.276871919631958\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 8 trained \n",
      "time to generate the model: 0.27601003646850586\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 9 trained \n",
      "time to generate the model: 0.2758452892303467\n",
      "Trained model saved to './result/trained_model.h5'\n",
      "Fold number 10 trained \n"
     ]
    }
   ],
   "source": [
    "n_class = len(np.unique(graph_labels))\n",
    "\n",
    "# Capsule Architecture Parameters:\n",
    "capsule_params = CapsuleParameters()\n",
    "\n",
    "# First conv layer: 'filters', kernel_size)\n",
    "conv_layer_name = 'conv_layer'\n",
    "conv_layer_params = {}\n",
    "conv_layer_params['filters'] = 256\n",
    "conv_layer_params['kernel_size'] = 9\n",
    "conv_layer_params['strides'] = [1, 1]\n",
    "conv_layer_params['padding'] = 'VALID'\n",
    "conv_layer_params['activation'] = 'relu'\n",
    "conv_layer_params['name'] = 'conv1'\n",
    "\n",
    "capsule_params.add_params(conv_layer_params, conv_layer_name)\n",
    "\n",
    "# First Capsule Layer:\n",
    "# [num_output_caps, caps_len,'filters',kernel_size,strides,padding]\n",
    "caps_layer_name = 'caps_layer'\n",
    "caps_layer_params = {}\n",
    "caps_layer_params['filters'] = 256\n",
    "caps_layer_params['kernel_size'] = 2\n",
    "caps_layer_params['strides'] = [2, 2]\n",
    "caps_layer_params['padding'] = 'VALID'\n",
    "caps_layer_params['padding'] = 'VALID'\n",
    "caps_layer_params['n_channels'] = 32\n",
    "caps_layer_params['dim_capsule'] = 8\n",
    "caps_layer_params['name'] = 'caps_layer'\n",
    "capsule_params.add_params(caps_layer_params, caps_layer_name)\n",
    "\n",
    "# Digit Capsule Layer:\n",
    "digit_layer_name = 'digitcaps_layer'\n",
    "digit_layer_params = {}\n",
    "digit_layer_params['n_channels'] = 10\n",
    "digit_layer_params['dim_capsule'] = 16\n",
    "digit_layer_params['name'] = 'digitcaps'\n",
    "capsule_params.add_params(digit_layer_params, digit_layer_name)\n",
    "\n",
    "# Capsule Decoder:\n",
    "decoder_layer = 'decoder_layer'\n",
    "decoder_params = {}\n",
    "decoder_params['first_dense'] = 256  # 250 #512\n",
    "decoder_params['second_dense'] = 512\n",
    "decoder_params['name'] = 'decoder'\n",
    "capsule_params.add_params(decoder_params, decoder_layer)\n",
    "\n",
    "# Training Hyperparameters:\n",
    "\n",
    "args_train = CapsuleTrainingParameters()\n",
    "args_train.batch_size = 50\n",
    "\n",
    "\n",
    "# Generate list of parameter sets::\n",
    "list_parameter_sets = []\n",
    "#list_parameter_sets.append(args_train)\n",
    "\n",
    "\n",
    "lr = 0.001  \n",
    "lr_decay = 0.9  \n",
    "lam_recon = 0.392\n",
    "\n",
    "training_params = CapsuleTrainingParameters(epochs=epochs,lr=lr,lr_decay=lr_decay,lam_recon=lam_recon)\n",
    "\n",
    "\n",
    "\n",
    "fold_set = []\n",
    "\n",
    "for j in range(n_folds):\n",
    "    fold_set.append(train_test_split(mutag_tensor2,\n",
    "                                     graph_labels,\n",
    "                                     test_size=0.10,\n",
    "                                     random_state=j))\n",
    "\n",
    "results_df = []\n",
    "val_acc = []\n",
    "training_time=[]\n",
    "\n",
    "for j in range(n_folds):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = fold_set[j]\n",
    "    data = ((x_train, y_train), (x_test, y_test))\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "\n",
    "    patchy_classifier = GraphClassifier(input_shape,n_class)\n",
    "    patchy_classifier.build_the_graph(capsule_params)\n",
    "    ##\n",
    "    #patchy_classifier.train_model.summary()\n",
    "    training_params.add_fold(j)\n",
    "    training_params.verbose = 0\n",
    "    ##\n",
    "    patchy_classifier.train(data, training_params)\n",
    "\n",
    "    training_time.append(patchy_classifier.training_time)\n",
    "    val_acc.append(patchy_classifier.results.val_capsnet_acc)\n",
    "\n",
    "    #if i == 0:\n",
    "    results_df.append(pd.DataFrame(patchy_classifier.results))\n",
    "    #else:\n",
    "    print('Fold number {} trained '.format(j + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789473652839661\n",
      "0.07080854598393455\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(pd.concat(results_df,1).loc['val_capsnet_acc',:]))\n",
    "print(np.std(pd.concat(results_df,1).loc['val_capsnet_acc',:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_list = nx_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h=1\n",
    "node_label=True\n",
    "n = len(nx_graphs)\n",
    "lists = [0] * n\n",
    "k = [0] * (h + 1)\n",
    "n_nodes = 0\n",
    "n_max = 0\n",
    "\n",
    "# Compute adjacency lists and n_nodes, the total number of\n",
    "# nodes in the dataset.\n",
    "for i in range(n):\n",
    "    lists[i] = nx.to_dict_of_lists(graph_list[i])\n",
    "    n_nodes = n_nodes + graph_list[i].number_of_nodes()\n",
    "\n",
    "    # Computing the maximum number of nodes in the graphs. It\n",
    "    # will be used in the computation of vectorial\n",
    "    # representation.\n",
    "    if(n_max < graph_list[i].number_of_nodes()):\n",
    "        n_max = graph_list[i].number_of_nodes()\n",
    "\n",
    "phi = np.zeros((n_max, n), dtype=np.uint64)\n",
    "\n",
    "# INITIALIZATION: initialize the nodes labels for each graph\n",
    "# with their labels or with degrees (for unlabeled graphs)\n",
    "\n",
    "labels = [0] * n\n",
    "label_lookup = {}\n",
    "label_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    l_aux = list(nx.get_node_attributes(graph_list[i],'attr_name'))\n",
    "    # It is assumed that the graph has an attribute\n",
    "    # 'node_label'\n",
    "    labels[i] = np.zeros(len(l_aux), dtype=np.int32)\n",
    "\n",
    "    for j in range(len(l_aux)):\n",
    "        if not (l_aux[j] in label_lookup):\n",
    "            label_lookup[l_aux[j]] = label_counter\n",
    "            labels[i][j] = label_counter\n",
    "            label_counter += 1\n",
    "        else:\n",
    "            labels[i][j] = label_lookup[l_aux[j]]\n",
    "        # labels are associated to a natural number\n",
    "        # starting with 0.\n",
    "        phi[labels[i][j], i] += 1\n",
    "    #print('graph num: {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = np.copy(phi.transpose())\n",
    "\n",
    "k = np.dot(phi.transpose(), phi)\n",
    "\n",
    "# MAIN LOOP\n",
    "it = 0\n",
    "new_labels = copy.deepcopy(labels)\n",
    "\n",
    "while it < h:\n",
    "    # create an empty lookup table\n",
    "    label_lookup = {}\n",
    "    label_counter = 0\n",
    "\n",
    "    phi = np.zeros((n_nodes, n), dtype=np.uint64)\n",
    "    for i in range(n):\n",
    "        for v in range(len(lists[i])):\n",
    "            # form a multiset label of the node v of the i'th graph\n",
    "            # and convert it to a string\n",
    "\n",
    "            long_label = np.concatenate((np.array([labels[i][v]]),\n",
    "                                         np.sort(labels[i]\n",
    "                                         [lists[i][v]])))\n",
    "            long_label_string = str(long_label)\n",
    "            # if the multiset label has not yet occurred, add it to the\n",
    "            # lookup table and assign a number to it\n",
    "            if not (long_label_string in label_lookup):\n",
    "                label_lookup[long_label_string] = label_counter\n",
    "                new_labels[i][v] = label_counter\n",
    "                label_counter += 1\n",
    "            else:\n",
    "                new_labels[i][v] = label_lookup[long_label_string]\n",
    "        # fill the column for i'th graph in phi\n",
    "        aux = np.bincount(new_labels[i])\n",
    "        try:\n",
    "\n",
    "            new_row = phi[new_labels[i], i] + aux[new_labels[i]]\n",
    "            new_row.astype(np.int64)\n",
    "            phi[new_labels[i], i] = new_row\n",
    "    \n",
    "\n",
    "        except:\n",
    "            print('type: phi' ,phi[new_labels[i], i])\n",
    "            print('type: aux' ,aux[new_labels[i]])\n",
    "            raise\n",
    "\n",
    "    k += np.dot(phi.transpose(), phi)\n",
    "    labels = copy.deepcopy(new_labels)\n",
    "    it = it + 1\n",
    "\n",
    "# Compute the normalized version of the kernel\n",
    "k_norm = np.zeros(k.shape)\n",
    "for i in range(k.shape[0]):\n",
    "    for j in range(k.shape[1]):\n",
    "        k_norm[i, j] = k[i, j] / np.sqrt(k[i, i] * k[j, j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in new_labels[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_graph = nx_graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.draw_spring(test_graph,with_labels = True,node_color='darkblue',font_color='white')\n",
    "#plt.savefig('graph_example.png')\n",
    "#nx.draw(test_sub,with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PatchyBuilder import ReceptiveFieldMaker\n",
    "import sys\n",
    "sys.path.append('PatchyTools/')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 18\n",
    "k = 10\n",
    "s = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PatchyBuilder import ReceptiveFieldMaker\n",
    "def indices_to_one_hot(number, nb_classes,label_dummy=-1):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    \n",
    "    if number==label_dummy:\n",
    "        return np.zeros(nb_classes)\n",
    "    else:\n",
    "        return np.eye(nb_classes)[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "for i in range(n):\n",
    "    rfMaker = ReceptiveFieldMaker(nx_graphs[i], w=w, k=k, s=1, labeling_procedure_name='betweeness',\n",
    "                              use_node_deg=False, one_hot=7,dummy_value=-1)\n",
    "    forcnn = rfMaker.make_()\n",
    "    #f =rfMaker.select_node_sequence()\n",
    "\n",
    "    train.append(np.array(forcnn).flatten().reshape(k*w,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_preprocessed = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_nodes(test_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_graph.add_node(44,gg='gege')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forcnn = []\n",
    "for graph in f:\n",
    "    frelabel = nx.relabel_nodes(graph,nx.get_node_attributes(graph, 'labeling'))  # rename the nodes wrt the labeling\n",
    "    sorted_idx = sorted(nx.get_node_attributes(frelabel, 'attr').items(), key=lambda x: x[0])\n",
    "    if rfMaker.one_hot > 0:\n",
    "        forcnn.append([utils.indices_to_one_hot(x[1], rfMaker.one_hot) for x in sorted_idx])\n",
    "        \n",
    "        #forcnn.append(pd.get_dummies(sorted_idx).values)\n",
    "    else:\n",
    "        forcnn.append(\n",
    "            [x[1] for x in sorted_idx])\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = -5\n",
    "graph = f[idx]\n",
    "print_nodes(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frelabel = nx.relabel_nodes(graph,nx.get_node_attributes(graph, 'labeling'))  # rename the nodes wrt the labeling\n",
    "sorted_idx = sorted(nx.get_node_attributes(frelabel, 'node_label').items(), key=lambda x: x[0])\n",
    "print(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_nodes(frelabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(forcnn))\n",
    "forcnn[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PatchyBuilder import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_graph = nx_graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frelabel=nx.relabel_nodes(test_graph,nx.get_node_attributes(test_graph,'node_label'))\n",
    "#nx.get_node_attributes(test_graph,'node_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in frelabel.nodes:\n",
    "    print(frelabel.node[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighborhood_assembly(vertex,nx_graph,k):\n",
    "    \"Output a set of neighbours of the vertex\"\n",
    "    N={vertex}\n",
    "    L={vertex}\n",
    "    while len(N)<k and len(L)>0:\n",
    "        tmp=set()\n",
    "        for v in L:\n",
    "            tmp=tmp.union(set(nx_graph.neighbors(v)))\n",
    "        L=tmp-N\n",
    "        #print('N: ',N)\n",
    "        #print(L)\n",
    "        N=N.union(L)\n",
    "    #print('last N: ',N)\n",
    "    return nx_graph.subgraph(list(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labeling_to_root(graph,vertex):\n",
    "    labeled_graph=nx.Graph(graph)\n",
    "    source_path_lengths = nx.single_source_dijkstra_path_length(graph, vertex)\n",
    "    nx.set_node_attributes(labeled_graph,source_path_lengths,'labeling')\n",
    "\n",
    "    return labeled_graph\n",
    "\n",
    "\n",
    "def compute_subgraph_ranking(subgraph,vertex,original_order_to_respect):\n",
    "    labeled_graph=nx.Graph(subgraph)\n",
    "    ordered_subgraph_from_centrality=labeling_to_root(subgraph,vertex)\n",
    "\n",
    "    all_labels_in_subgraph_dict=nx.get_node_attributes(ordered_subgraph_from_centrality,'labeling')\n",
    "\n",
    "    new_ordered_dict=rank_label_wrt_dict(ordered_subgraph_from_centrality,all_labels_in_subgraph_dict,original_order_to_respect)\n",
    "\n",
    "    nx.set_node_attributes(labeled_graph,new_ordered_dict,'labeling') \n",
    "\n",
    "\n",
    "    return labeled_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered_subgraph_from_centrality=labeling_to_root(test_sub,vertex)\n",
    "all_labels_in_subgraph_dict=nx.get_node_attributes(ordered_subgraph_from_centrality,'labeling')\n",
    "print_dict(all_labels_in_subgraph_dict)\n",
    "#new_ordered_dict=rank_label_wrt_dict(ordered_subgraph_from_centrality,all_labels_in_subgraph_dict,original_order_to_respect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict = all_labels_in_subgraph_dict\n",
    "dict_to_respect = original_order_to_respect\n",
    "all_distinc_labels=list(set(label_dict.values()))\n",
    "new_ordered_dict=label_dict     \n",
    "\n",
    "latest_biggest_label=0\n",
    "\n",
    "for label in all_distinc_labels:\n",
    "    #print(label)\n",
    "    nodes_with_this_label = [x for x,y in ordered_subgraph_from_centrality.nodes(data=True) if y['labeling']==label]\n",
    "    print('nodes ', nodes_with_this_label)\n",
    "    if len(nodes_with_this_label)>=2:\n",
    "\n",
    "        inside_ordering=sorted(nodes_with_this_label, key=dict_to_respect.get)\n",
    "        inside_order_dict=dict(zip(inside_ordering,range(len(inside_ordering))))\n",
    "    \n",
    "        for k,v in inside_order_dict.items():\n",
    "\n",
    "            new_ordered_dict[k]=latest_biggest_label+1+inside_order_dict[k]\n",
    "\n",
    "        latest_biggest_label=latest_biggest_label+len(nodes_with_this_label)\n",
    "\n",
    "    else:\n",
    "        new_ordered_dict[nodes_with_this_label[0]]=latest_biggest_label+1 \n",
    "        latest_biggest_label=latest_biggest_label+1\n",
    "    try:\n",
    "        print('inside: ', inside_ordering)\n",
    "        print('new: ', new_ordered_dict)\n",
    "        \n",
    "    except:\n",
    "\n",
    "        pass\n",
    "    print(label,nodes_with_this_label,latest_biggest_label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_dict(new_ordered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inside_ordering=sorted(nodes_with_this_label, key=dict_to_respect.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for label in all_distinc_labels:\n",
    "\n",
    "    nodes_with_this_label = [x for x,y in test_sub.nodes(data=True) if y['labeling']==label]\n",
    "    print(label,nodes_with_this_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subgraph_U=compute_subgraph_ranking(test_sub,vertex,original_order_to_respect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_nodes(test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_nodes(subgraph_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(subgraph_U.nodes())>10:\n",
    "\n",
    "    d=dict(nx.get_node_attributes(subgraph_U,'labeling'))    \n",
    "    k_first_nodes=sorted(d,key=d.get)[0:k]\n",
    "    subgraph_N=subgraph_U.subgraph(k_first_nodes)  \n",
    "\n",
    "    ranked_subgraph_by_labeling_procedure=labeling_procedure(test_sub)['labeled_graph']\n",
    "    original_order_to_respect=nx.get_node_attributes(ranked_subgraph_by_labeling_procedure,'labeling')        \n",
    "    subgraph_ranked_N=compute_subgraph_ranking(subgraph_N,vertex,original_order_to_respect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labeling_procedure(graph):   \n",
    "    \n",
    "    a=betweenness_centrality_labeling(graph)\n",
    "    return a\n",
    "\n",
    "def betweenness_centrality_labeling(graph,approx=None):\n",
    "    result={}\n",
    "    #labeled_graph=nx.Graph(graph)\n",
    "    \n",
    "    if approx is None:\n",
    "        centrality=list(nx.betweenness_centrality(graph).items())\n",
    "    else:\n",
    "        centrality=list(nx.betweenness_centrality(graph,k=approx).items())\n",
    "    sorted_centrality=sorted(centrality,key=lambda n:n[1],reverse=True)\n",
    "    dict_={}\n",
    "    label=0\n",
    "    for t in sorted_centrality:\n",
    "        dict_[t[0]]=label\n",
    "        label+=1\n",
    "    nx.set_node_attributes(graph,dict_,'labeling')\n",
    "    ordered_nodes=list(zip(*sorted_centrality))[0]\n",
    "\n",
    "    result['labeled_graph']=graph\n",
    "    result['sorted_centrality']=sorted_centrality\n",
    "    result['ordered_nodes']=ordered_nodes\n",
    "    return result\n",
    "\n",
    "def rank_label_wrt_dict(subgraph,label_dict,dict_to_respect):\n",
    "\n",
    "    all_distinc_labels=list(set(label_dict.values()))\n",
    "    new_ordered_dict=label_dict     \n",
    "\n",
    "    latest_biggest_label=0\n",
    "\n",
    "    for label in all_distinc_labels:\n",
    "\n",
    "        nodes_with_this_label = [x for x,y in subgraph.nodes(data=True) if y['labeling']==label]\n",
    "\n",
    "        if len(nodes_with_this_label)>=2:\n",
    "\n",
    "            inside_ordering=sorted(nodes_with_this_label, key=dict_to_respect.get)\n",
    "            inside_order_dict=dict(zip(inside_ordering,range(len(inside_ordering))))\n",
    "\n",
    "            for k,v in inside_order_dict.items():\n",
    "\n",
    "                new_ordered_dict[k]=latest_biggest_label+1+inside_order_dict[k]\n",
    "\n",
    "            latest_biggest_label=latest_biggest_label+len(nodes_with_this_label)\n",
    "\n",
    "        else :\n",
    "            new_ordered_dict[nodes_with_this_label[0]]=latest_biggest_label+1 \n",
    "            latest_biggest_label=latest_biggest_label+1\n",
    "\n",
    "    return new_ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_graph(subgraph,vertex):\n",
    "\n",
    "    \"U set of vertices. Return le receptive field du vertex (un graph normalisé)\"\n",
    "    ranked_subgraph_by_labeling_procedure=betweenness_centrality_labeling(subgraph)['labeled_graph']\n",
    "    original_order_to_respect=nx.get_node_attributes(ranked_subgraph_by_labeling_procedure,'labeling') # à changer je pense\n",
    "    subgraph_U=compute_subgraph_ranking(subgraph,vertex,original_order_to_respect) #ordonne les noeuds w.r.t labeling procedure\n",
    "\n",
    "    if len(subgraph_U.nodes())>10:\n",
    "\n",
    "        d=dict(nx.get_node_attributes(subgraph_U,'labeling'))    \n",
    "        k_first_nodes=sorted(d,key=d.get)[0:k]\n",
    "        subgraph_N=subgraph_U.subgraph(k_first_nodes)  \n",
    "\n",
    "        ranked_subgraph_by_labeling_procedure=labeling_procedure(subgraph)['labeled_graph']\n",
    "        original_order_to_respect=nx.get_node_attributes(ranked_subgraph_by_labeling_procedure,'labeling')        \n",
    "        subgraph_ranked_N=compute_subgraph_ranking(subgraph_N,vertex,original_order_to_respect)\n",
    "\n",
    "    elif len(subgraph_U.nodes())<k:\n",
    "        subgraph_ranked_N=add_dummy_nodes_at_the_end(subgraph_U)\n",
    "    else :\n",
    "        subgraph_ranked_N=subgraph_U\n",
    "\n",
    "    return canonicalizes(subgraph_ranked_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertex = 0\n",
    "test_sub = neighborhood_assembly(vertex,test_graph,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_subgraph_by_labeling_procedure=betweenness_centrality_labeling(test_sub)['labeled_graph']\n",
    "original_order_to_respect=nx.get_node_attributes(ranked_subgraph_by_labeling_procedure,'labeling') # à changer je pense\n",
    "#subgraph_U=compute_subgraph_ranking(test_sub,vertex,original_order_to_respect) #ordonne les noeuds w.r.t labeling procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.get_node_attributes(ranked_subgraph_by_labeling_procedure,'labeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in ranked_subgraph_by_labeling_procedure.nodes:\n",
    "    print(i,ranked_subgraph_by_labeling_procedure.node[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_order_to_respect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "centrality=list(nx.betweenness_centrality(test_sub).items())\n",
    "sorted_centrality = sorted(centrality,key=lambda n:n[1],reverse=True)\n",
    "sorted_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_={}\n",
    "label=0\n",
    "for t in sorted_centrality:\n",
    "    dict_[t[0]]=label\n",
    "    label+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "\n",
    "class GK_WL():\n",
    "    \"\"\"\n",
    "    Weisfeiler_Lehman graph kernel.\n",
    "    \"\"\"\n",
    "    def compare_list(self, graph_list, h=1, node_label=True):\n",
    "        \"\"\"Compute the all-pairs kernel values for a list of graphs.\n",
    "\n",
    "        This function can be used to directly compute the kernel\n",
    "        matrix for a list of graphs. The direct computation of the\n",
    "        kernel matrix is faster than the computation of all individual\n",
    "        pairwise kernel values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph_list: list\n",
    "            A list of graphs (list of networkx graphs)\n",
    "        h : interger\n",
    "            Number of iterations.\n",
    "        node_label : boolean\n",
    "            Whether to use original node labels. True for using node labels\n",
    "            saved in the attribute 'node_label'. False for using the node\n",
    "            degree of each node as node attribute.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        K: numpy.array, shape = (len(graph_list), len(graph_list))\n",
    "        The similarity matrix of all graphs in graph_list.\n",
    "\n",
    "        \"\"\"\n",
    "        self.graphs = graph_list\n",
    "        n = len(graph_list)\n",
    "        lists = [0] * n\n",
    "        k = [0] * (h + 1)\n",
    "        n_nodes = 0\n",
    "        n_max = 0\n",
    "\n",
    "        # Compute adjacency lists and n_nodes, the total number of\n",
    "        # nodes in the dataset.\n",
    "        for i in range(n):\n",
    "            lists[i] = graph_list[i].adjacency_list()\n",
    "            n_nodes = n_nodes + graph_list[i].number_of_nodes()\n",
    "\n",
    "            # Computing the maximum number of nodes in the graphs. It\n",
    "            # will be used in the computation of vectorial\n",
    "            # representation.\n",
    "            if(n_max < graph_list[i].number_of_nodes()):\n",
    "                n_max = graph_list[i].number_of_nodes()\n",
    "\n",
    "        phi = np.zeros((n_max, n), dtype=np.uint64)\n",
    "\n",
    "        # INITIALIZATION: initialize the nodes labels for each graph\n",
    "        # with their labels or with degrees (for unlabeled graphs)\n",
    "\n",
    "        labels = [0] * n\n",
    "        label_lookup = {}\n",
    "        label_counter = 0\n",
    "\n",
    "        # label_lookup is an associative array, which will contain the\n",
    "        # mapping from multiset labels (strings) to short labels\n",
    "        # (integers)\n",
    "\n",
    "        if node_label is True:\n",
    "            for i in range(n):\n",
    "                l_aux = nx.get_node_attributes(graph_list[i],\n",
    "                                               'node_label').values()\n",
    "                # It is assumed that the graph has an attribute\n",
    "                # 'node_label'\n",
    "                labels[i] = np.zeros(len(l_aux), dtype=np.int32)\n",
    "\n",
    "                for j in range(len(l_aux)):\n",
    "                    if not (l_aux[j] in label_lookup):\n",
    "                        label_lookup[l_aux[j]] = label_counter\n",
    "                        labels[i][j] = label_counter\n",
    "                        label_counter += 1\n",
    "                    else:\n",
    "                        labels[i][j] = label_lookup[l_aux[j]]\n",
    "                    # labels are associated to a natural number\n",
    "                    # starting with 0.\n",
    "                    phi[labels[i][j], i] += 1\n",
    "        else:\n",
    "            for i in range(n):\n",
    "                labels[i] = np.array(graph_list[i].degree().values())\n",
    "                for j in range(len(labels[i])):\n",
    "                    phi[labels[i][j], i] += 1\n",
    "\n",
    "        # Simplified vectorial representation of graphs (just taking\n",
    "        # the vectors before the kernel iterations), i.e., it is just\n",
    "        # the original nodes degree.\n",
    "        self.vectors = np.copy(phi.transpose())\n",
    "\n",
    "        k = np.dot(phi.transpose(), phi)\n",
    "\n",
    "        # MAIN LOOP\n",
    "        it = 0\n",
    "        new_labels = copy.deepcopy(labels)\n",
    "\n",
    "        while it < h:\n",
    "            # create an empty lookup table\n",
    "            label_lookup = {}\n",
    "            label_counter = 0\n",
    "\n",
    "            phi = np.zeros((n_nodes, n), dtype=np.uint64)\n",
    "            for i in range(n):\n",
    "                for v in range(len(lists[i])):\n",
    "                    # form a multiset label of the node v of the i'th graph\n",
    "                    # and convert it to a string\n",
    "\n",
    "                    long_label = np.concatenate((np.array([labels[i][v]]),\n",
    "                                                 np.sort(labels[i]\n",
    "                                                 [lists[i][v]])))\n",
    "                    long_label_string = str(long_label)\n",
    "                    # if the multiset label has not yet occurred, add it to the\n",
    "                    # lookup table and assign a number to it\n",
    "                    if not (long_label_string in label_lookup):\n",
    "                        label_lookup[long_label_string] = label_counter\n",
    "                        new_labels[i][v] = label_counter\n",
    "                        label_counter += 1\n",
    "                    else:\n",
    "                        new_labels[i][v] = label_lookup[long_label_string]\n",
    "                # fill the column for i'th graph in phi\n",
    "                aux = np.bincount(new_labels[i])\n",
    "                phi[new_labels[i], i] += aux[new_labels[i]]\n",
    "\n",
    "            k += np.dot(phi.transpose(), phi)\n",
    "            labels = copy.deepcopy(new_labels)\n",
    "            it = it + 1\n",
    "\n",
    "        # Compute the normalized version of the kernel\n",
    "        k_norm = np.zeros(k.shape)\n",
    "        for i in range(k.shape[0]):\n",
    "            for j in range(k.shape[1]):\n",
    "                k_norm[i, j] = k[i, j] / np.sqrt(k[i, i] * k[j, j])\n",
    "\n",
    "        return k_norm\n",
    "\n",
    "    def compare(self, g_1, g_2, h=1, node_label=True):\n",
    "        \"\"\"Compute the kernel value (similarity) between two graphs.\n",
    "        The kernel is normalized to [0,1] by the equation:\n",
    "        k_norm(g1, g2) = k(g1, g2) / sqrt(k(g1,g1) * k(g2,g2))\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g_1 : networkx.Graph\n",
    "            First graph.\n",
    "        g_2 : networkx.Graph\n",
    "            Second graph.\n",
    "        h : interger\n",
    "            Number of iterations.\n",
    "        node_label : boolean\n",
    "            Whether to use the values under the graph attribute 'node_label'\n",
    "            as node labels. If False, the degree of the nodes are used as\n",
    "            labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        k : The similarity value between g1 and g2.\n",
    "        \"\"\"\n",
    "        gl = [g_1, g_2]\n",
    "        return self.compare_list(gl, h, node_label)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wl = GK_WL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wl.compare_list(nx_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Capsule Architecture Parameters:\n",
    "capsule_params = CapsuleParameters()\n",
    "\n",
    "# First conv layer: 'filters', kernel_size)\n",
    "conv_layer_name = 'conv_layer'\n",
    "conv_layer_params = {}\n",
    "conv_layer_params['filters'] = 256\n",
    "conv_layer_params['kernel_size'] = 9\n",
    "conv_layer_params['strides'] = [1, 1]\n",
    "conv_layer_params['padding'] = 'VALID'\n",
    "conv_layer_params['activation'] = 'relu'\n",
    "conv_layer_params['name'] = 'conv1'\n",
    "\n",
    "capsule_params.add_params(conv_layer_params, conv_layer_name)\n",
    "\n",
    "# First Capsule Layer:\n",
    "# [num_output_caps, caps_len,'filters',kernel_size,strides,padding]\n",
    "caps_layer_name = 'caps_layer'\n",
    "caps_layer_params = {}\n",
    "caps_layer_params['filters'] = 256\n",
    "caps_layer_params['kernel_size'] = 2\n",
    "caps_layer_params['strides'] = [2, 2]\n",
    "caps_layer_params['padding'] = 'VALID'\n",
    "caps_layer_params['padding'] = 'VALID'\n",
    "caps_layer_params['n_channels'] = 32\n",
    "caps_layer_params['dim_capsule'] = 8\n",
    "caps_layer_params['name'] = 'caps_layer'\n",
    "capsule_params.add_params(caps_layer_params, caps_layer_name)\n",
    "\n",
    "# Digit Capsule Layer:\n",
    "digit_layer_name = 'digitcaps_layer'\n",
    "digit_layer_params = {}\n",
    "digit_layer_params['n_channels'] = 10\n",
    "digit_layer_params['dim_capsule'] = 16\n",
    "digit_layer_params['name'] = 'digitcaps'\n",
    "capsule_params.add_params(digit_layer_params, digit_layer_name)\n",
    "\n",
    "# Capsule Decoder:\n",
    "decoder_layer = 'decoder_layer'\n",
    "decoder_params = {}\n",
    "decoder_params['first_dense'] = 256  # 250 #512\n",
    "decoder_params['second_dense'] = 512\n",
    "decoder_params['name'] = 'decoder'\n",
    "capsule_params.add_params(decoder_params, decoder_layer)\n",
    "\n",
    "# Training Hyperparameters:\n",
    "\n",
    "args_train = CapsuleTrainingParameters()\n",
    "data_split = train_test_split(graph_tensor,graph_labels,test_size=0.10,random_state =0)\n",
    "x_train, x_test, y_train, y_test = data_split\n",
    "data = ((x_train, y_train), (x_test, y_test))\n",
    "input_shape = x_train.shape[1:]\n",
    "n_class = len(np.unique(y_train))\n",
    "patchy_classifier = GraphClassifier(input_shape,n_class)\n",
    "patchy_classifier.build_the_graph(capsule_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
